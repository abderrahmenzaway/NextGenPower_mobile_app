# NILM Backend API - Complete Setup Guide

A complete Express.js + Python Flask backend for serving NILM (Non-Intrusive Load Monitoring) model predictions via REST API.

## ðŸ“¦ What's Included

```
backend_api/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.js           # Express server (entry point)
â”‚   â”œâ”€â”€ routes.js           # API routes & endpoints
â”‚   â”œâ”€â”€ pythonClient.js     # Client for Python Flask service
â”‚   â”œâ”€â”€ validation.js       # Request validation (Joi schema)
â”‚   â””â”€â”€ logger.js           # Centralized logging
â”œâ”€â”€ python_service/
â”‚   â”œâ”€â”€ model_service.py    # Flask API for model inference
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ Dockerfile          # Docker configuration (optional)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.js           # Centralized configuration
â”œâ”€â”€ package.json            # Node.js dependencies
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ test.js                 # API test suite
â”œâ”€â”€ setup.sh / setup.bat    # Automated setup script
â”œâ”€â”€ README.md               # Full documentation
â”œâ”€â”€ API_EXAMPLES.md         # cURL & Python examples
â””â”€â”€ DEPLOYMENT.md           # Production deployment guide
```

## ðŸš€ Quick Start (5 minutes)

### On Windows

```cmd
# 1. Open terminal in backend_api folder
cd c:\Users\chehin\Desktop\app_class\mobile-app\backend_formodel\backend_api

# 2. Run setup script
setup.bat

# 3. Create .env file
copy .env.example .env

# 4. Start Python service (Terminal 1)
cd python_service
venv\Scripts\activate.bat
python model_service.py

# 5. Start Express API (Terminal 2)
cd ..
npm start

# 6. Test the API (Terminal 3)
curl http://localhost:3001/api/health
```

### On macOS/Linux

```bash
# 1. Navigate to backend_api folder
cd backend_formodel/backend_api

# 2. Run setup script
bash setup.sh

# 3. Create .env file
cp .env.example .env

# 4. Start Python service (Terminal 1)
cd python_service
source venv/bin/activate
python model_service.py

# 5. Start Express API (Terminal 2)
cd ..
npm start

# 6. Test the API (Terminal 3)
curl http://localhost:3001/api/health
```

## ðŸ“‹ Prerequisites

- **Node.js 16+**: [Download](https://nodejs.org/)
- **Python 3.9+**: [Download](https://www.python.org/)
- **CUDA Toolkit (optional)**: For GPU acceleration
- **4GB RAM minimum**: More if using GPU
- **Ports 3001 and 5001** available

## ðŸ”§ Detailed Setup Instructions

### Step 1: Install Node.js Dependencies

```bash
cd backend_api
npm install
```

This installs:
- `express` - Web framework
- `cors` - Cross-origin resource sharing
- `axios` - HTTP client for Python service
- `joi` - Data validation
- `uuid` - Request ID generation
- `dotenv` - Environment variable management

### Step 2: Set Up Python Environment

```bash
cd python_service

# Create virtual environment
python -m venv venv

# Activate (choose based on your OS)
# Windows:
venv\Scripts\activate.bat
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

This installs:
- `torch` - PyTorch for model inference
- `numpy` - Array operations
- `flask` - Web framework
- `flask-cors` - CORS support
- `scikit-learn` - Preprocessing (StandardScaler)
- `pandas` - Data handling

### Step 3: Configure Environment

```bash
# Copy template
cp .env.example .env

# Edit .env with your settings
# Key configurations:
# - EXPRESS_PORT: Node.js API port (default: 3001)
# - FLASK_PORT: Python service port (default: 5001)
# - MODEL_NAME: Which model to use (TCN_best.pth, BiLSTM_best.pth, ATCN_best.pth)
# - MODEL_PATH: Path to saved models (default: ../NILM_SIDED/saved_models)
```

### Step 4: Verify Model Files

Ensure model files exist:

```bash
ls -la ../NILM_SIDED/saved_models/
# Should show:
# - TCN_best.pth
# - BiLSTM_best.pth (if trained)
# - ATCN_best.pth (if trained)
```

## ðŸŽ¯ Usage

### Terminal 1: Start Python Flask Service

```bash
cd backend_api/python_service

# Activate environment
venv\Scripts\activate  # Windows
# or
source venv/bin/activate  # macOS/Linux

# Start service
python model_service.py
```

Expected output:
```
âœ… Model loaded successfully
ðŸ“ Starting Flask server on port 5001
```

### Terminal 2: Start Express API Server

```bash
cd backend_api

# Install dependencies if not done
npm install

# Start server
npm start
```

Expected output:
```
âœ… Express API listening on port 3001
ðŸ“ Access at http://localhost:3001
```

### Terminal 3: Test the API

```bash
# Health check
curl http://localhost:3001/api/health

# Service status
curl http://localhost:3001/api/status

# Make a prediction
curl -X POST http://localhost:3001/api/predict \
  -H "Content-Type: application/json" \
  -d '{
    "aggregate_sequence": [150.5, 152.1, ..., 160.2]
  }'
```

See `API_EXAMPLES.md` for more examples.

## ðŸ“¡ API Endpoints

### Health & Status

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/health` | Health check |
| GET | `/api/status` | Service status |
| GET | `/api/config` | Configuration (dev only) |

### Prediction

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/predict` | Run NILM prediction |

**Request Format:**
```json
{
  "request_id": "optional_string",
  "timestamp": "ISO8601_optional",
  "aggregate_sequence": [150.5, 152.1, ..., 160.2]  // Exactly 288 values
}
```

**Response Format:**
```json
{
  "request_id": "req_12345",
  "timestamp": "2025-11-23T14:30:00Z",
  "predictions": {
    "EVSE": 45.2,
    "PV": -12.5,
    "CS": 22.1,
    "CHP": 0.0,
    "BA": 5.3
  },
  "status": "success",
  "processingTimeMs": 125
}
```

## âœ… Testing

### Automated Test Suite

```bash
# Install test dependencies
npm install --save-dev chalk

# Run tests
node test.js

# Or test specific endpoint
node test.js health
```

### Manual Testing with cURL

See `API_EXAMPLES.md` for comprehensive examples.

### Testing with Python

```python
import requests
import numpy as np

# Generate test data
data = np.random.rand(288).tolist()

response = requests.post(
    'http://localhost:3001/api/predict',
    json={'aggregate_sequence': data}
)

print(response.json()['predictions'])
```

## ðŸ” Troubleshooting

### Issue: "Cannot connect to Flask service"

**Solution:**
```bash
# Ensure Flask service is running
# Terminal 1: python model_service.py

# Check if port 5001 is in use
lsof -i :5001  # macOS/Linux
netstat -ano | findstr :5001  # Windows

# Update FLASK_SERVICE_URL in .env
FLASK_SERVICE_URL=http://localhost:5001
```

### Issue: "Port 3001/5001 already in use"

**Solution:**
```bash
# Change ports in .env
EXPRESS_PORT=3002
FLASK_PORT=5002

# Or kill existing process
lsof -i :3001 | grep LISTEN | awk '{print $2}' | xargs kill -9
```

### Issue: "Model not found"

**Solution:**
```bash
# Verify model path
# Check if TCN_best.pth exists:
ls ../NILM_SIDED/saved_models/TCN_best.pth

# Update MODEL_PATH in .env
MODEL_PATH=../NILM_SIDED/saved_models
```

### Issue: "CUDA out of memory" or slow inference

**Solutions:**
1. Use CPU instead of GPU:
   ```python
   device = torch.device('cpu')
   ```

2. Reduce batch size (in model_service.py)

3. Check available GPU memory:
   ```bash
   nvidia-smi
   ```

## ðŸ“Š Architecture

```
Your Mobile App
    â†“ (HTTP POST)
Express API Server (Node.js)
    â”œâ”€ Validates request (schema, format, values)
    â”œâ”€ Calls Flask service
    â””â”€ Returns formatted response
    â†“ (HTTP POST)
Flask Service (Python)
    â”œâ”€ Receives aggregate sequence
    â”œâ”€ Normalizes with StandardScaler
    â”œâ”€ Runs PyTorch model
    â”œâ”€ Inverse transforms predictions
    â””â”€ Returns predictions JSON
    â†“
Express API Server
    â””â”€ Returns to mobile app
```

## ðŸ” Security

### CORS Configuration

Edit `.env`:
```env
# Allow specific domain
CORS_ORIGIN=https://your-frontend-domain.com

# Or allow all (development only)
CORS_ORIGIN=*
```

### API Key Authentication (Optional)

See `DEPLOYMENT.md` for adding API key protection.

### HTTPS in Production

See `DEPLOYMENT.md` for SSL/TLS setup.

## ðŸ“ˆ Performance Tips

1. **Use GPU** if available for faster inference
2. **Cache predictions** if same sequences are requested repeatedly
3. **Run multiple Flask instances** behind a load balancer for scaling
4. **Monitor resource usage** (CPU, memory, GPU)
5. **Set appropriate timeouts** in production

## ðŸ“š Documentation

- **README.md** - Full API documentation
- **API_EXAMPLES.md** - cURL, Python, JavaScript examples
- **DEPLOYMENT.md** - Production setup (Docker, AWS, Heroku)
- **API_CONTRACT.md** (NILM_SIDED) - Model specifications

## ðŸ› ï¸ Model Configuration

The API is pre-configured for `TCN_best.pth`. To use different models:

```bash
# Edit .env
MODEL_NAME=BiLSTM_best.pth
# or
MODEL_NAME=ATCN_best.pth
```

Then update `python_service/model_service.py` to load the correct architecture:

```python
if MODEL_NAME == 'TCN_best.pth':
    model = TCNModel(...)
elif MODEL_NAME == 'BiLSTM_best.pth':
    model = BiLSTMModel(...)
elif MODEL_NAME == 'ATCN_best.pth':
    model = ATCNModel(...)
```

## ðŸ”„ Development Workflow

1. **Make code changes** to `src/` or `python_service/`
2. **Restart services** (Ctrl+C and npm start / python model_service.py)
3. **Test changes** with curl or test suite
4. **Check logs** for errors

Enable auto-restart with:
```bash
npm install -D nodemon
npm run dev
```

## ðŸ“ž Support & Issues

1. **Check logs** - Both services log errors
2. **Test endpoints** - Use curl to test directly
3. **Verify configuration** - Check .env and config.js
4. **Check connectivity** - Ensure both services can reach each other
5. **Review examples** - See API_EXAMPLES.md for working requests

## ðŸŽ‰ Next Steps

1. âœ… Verify API is working (test endpoints)
2. ðŸ“± Integrate with mobile app (replace `http://localhost:3001` with actual server)
3. ðŸš€ Deploy to production (see DEPLOYMENT.md)
4. ðŸ“Š Monitor performance and logs
5. ðŸ”„ Optimize based on usage patterns

## ðŸ“ Quick Reference

```bash
# Start everything
terminal1> cd python_service && python model_service.py
terminal2> npm start

# Test
curl http://localhost:3001/api/health

# View logs
terminal1-2: Console output (watch for errors)

# Stop
Ctrl+C in each terminal

# Change port
# Edit .env:
EXPRESS_PORT=3002
FLASK_PORT=5002
```

---

**You're all set!** ðŸŽ‰ The NILM backend API is ready to serve predictions to your mobile app.

For detailed API usage, see `README.md`.
For production deployment, see `DEPLOYMENT.md`.
For request examples, see `API_EXAMPLES.md`.
